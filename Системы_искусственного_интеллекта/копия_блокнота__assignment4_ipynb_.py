# -*- coding: utf-8 -*-
"""Копия блокнота "assignment4.ipynb"

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ph4lmTMWKiH_0iQMwwkKsB5NCWP9Rc8m

# Лабораторная работа 4

## Практика TensorFlow 2.x

### Задание

В данной лабораторной работе будет практиковаться материал из лекций по TensorFlow на примере решения задачи классификации изображений из датасета Fashion MNIST. Fashion MNIST - набор черно-белых изображений размера 28х28, что поделены на 10 классов. Будет построена с нуля и обучена модель логистической регрессии используя как низкоуровневый API. Опционально решение той же задачи нейронной сетью, построенной и обученной с использованием Keras.

0 Базовые операции.
   - 0.1 Сгенерируйте три непересекающихся гауссовых облака, используя `tf.random`. Визуализируйте их разными цветами, используя matplotlib.
   - 0.2 Сгенерируйте случайную матрицу 2x2, умножьте каждую точку в облаках на эту матрицу. Повторно визуализируйте облака.
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

# Задаем параметры для гауссовых облаков
num_points = 100
mean1, mean2, mean3 = [2, 3], [8, 8], [14, 3]
cov = [[1, 0.5], [0.5, 1]]  #ковариационной матрицы

# Генерируем случайные точки для каждого гауссова облака
cholesky1 = tf.linalg.cholesky(cov)
cloud1 = tf.matmul(tf.random.normal((num_points, 2)), cholesky1) + mean1
cloud2 = tf.matmul(tf.random.normal((num_points, 2)), cholesky1) + mean2
cloud3 = tf.matmul(tf.random.normal((num_points, 2)), cholesky1) + mean3

# Визуализация исходных гауссовых облаков
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.scatter(cloud1[:, 0], cloud1[:, 1], label='Cloud 1', alpha=0.5)
plt.scatter(cloud2[:, 0], cloud2[:, 1], label='Cloud 2', alpha=0.5)
plt.scatter(cloud3[:, 0], cloud3[:, 1], label='Cloud 3', alpha=0.5)
plt.title('Исходные гауссовы облака')
plt.xlabel('X-axis')
plt.ylabel('Y-axis')
plt.legend()

# Генерируем случайную матрицу 2x2
random_matrix = np.random.rand(2, 2)

# Умножаем каждую точку в облаках на случайную матрицу
cloud1_transformed = tf.matmul(cloud1, random_matrix)
cloud2_transformed = tf.matmul(cloud2, random_matrix)
cloud3_transformed = tf.matmul(cloud3, random_matrix)

# Визуализация преобразованных гауссовых облаков
plt.subplot(1, 2, 2)
plt.scatter(cloud1_transformed[:, 0], cloud1_transformed[:, 1], label='Cloud 1', alpha=0.5)
plt.scatter(cloud2_transformed[:, 0], cloud2_transformed[:, 1], label='Cloud 2', alpha=0.5)
plt.scatter(cloud3_transformed[:, 0], cloud3_transformed[:, 1], label='Cloud 3', alpha=0.5)
plt.title('Преобразованные гауссовы облака')
plt.xlabel('X-axis')
plt.ylabel('Y-axis')
plt.legend()

plt.tight_layout()
plt.show()

"""1 Загрузите датасет Fashion MNIST, используя метод из [tf.keras.datasets](https://www.tensorflow.org/api_docs/python/tf/keras/datasets). Визуализируйте несколько изображений, используя библиотеку matplotlib."""

fashion_mnist = tf.keras.datasets.fashion_mnist

(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()

# Имена классов
class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

# Визуализация примера изображения
plt.figure()
plt.imshow(X_train[0], cmap='gray')
plt.colorbar()
plt.grid(False)
plt.show()

"""2 Нормализуйте данные либо средним и СКО, либо приведя значения пикселей в интервал [-1, 1]."""

# Нормализация данных, приведя значения пикселей в интервал [-1, 1]
d = 255.0 / 2.0

X_train = (X_train / d) - 1.0
X_test = (X_test / d) - 1.0

# Визуализация примера изображения
plt.figure()
plt.imshow(X_train[0], cmap='gray')
plt.colorbar()
plt.grid(False)
plt.show()

"""3 Создайте обучающий и тестовый TensorFlow Dataset из нормализованного набора данных. К обучающему датасету добавьте перемешивание (`.shuffle`). К обоим датасетам добавьте генерацию батчей размера 128 (`.batch(128)`)."""

# Создание TensorFlow Dataset для обучения и тестирования
train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))
test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))

# Перемешивание и создание батчей для обучающего датасета
train_dataset = train_dataset.shuffle(buffer_size=len(X_train)).batch(128)

# Создание батчей для датасетов
test_dataset = test_dataset.batch(128)

"""4 Напишите класс слоя `Flatten`, что трансформирует изображения из матричного в векторный вид: [28, 28] -> [28 * 28]. Класс слоя должен наследоваться от `tf.Module`. Для трансформации тензоров изображений использовать `tf.reshape`."""

class FlattenLayer(tf.Module):
    def __init__(self):
        pass  # Нет необходимости в дополнительной инициализации

    def __call__(self, inputs):
        # Используем tf.reshape для трансформации изображения
        flattened = tf.reshape(inputs, shape=(-1, tf.reduce_prod(inputs.shape[1:])))
        return flattened

# Пример использования FlattenLayer
# Создание экземпляра FlattenLayer
flatten_layer = FlattenLayer()

# Преобразование изображений из матричного векторный вид
input_image = X_train[0]  # Возьмем первое изображение
flattened_image = flatten_layer(input_image[None, ...])  # None добавляет размерность батча

print("Размерность до Flatten:", input_image.shape)
print("Размерность после Flatten:", flattened_image.shape)

"""5 Напишите класс линейного слоя `Linear`, что умножает входной вектор на матрицу весов. Класс слоя должен наследоваться от `tf.Module`. Матрицу весов хранить в `tf.Variable`. Параметры конструктора: `in_d` - размерность входного вектора, `out_d` - размерность выходного вектора. В качестве начальной инициализации весов использовать гауссово распределение с СКО=0.01."""

class Linear(tf.Module):
    def __init__(self, in_d, out_d):
        super(Linear, self).__init__()
        self.weights = tf.Variable(tf.random.normal([in_d, out_d], stddev=0.01), name='weights')
        self.bias = tf.Variable(tf.zeros([out_d]), name='bias')

    def __call__(self, inputs):
        return tf.matmul(inputs, self.weights) + self.bias


# Пример использования Linear
# Создание экземпляра линейного слоя
linear_layer = Linear(in_d=10, out_d=5)  # Пример для входной размерности 10 и выходной размерности 5

# Применение линейного слоя к случайному входному тензору размерности (batch_size, in_d)
input_data = tf.random.normal([24, 10])  # Пример для батча размером 24 и входной размерности 10
output = linear_layer(input_data)  # Применение линейного слоя к входным данным

print("Размерность входных данных:", input_data.shape)
print("Размерность выходных данных:", output.shape)

"""6 Напишите класс модели логистической регрессии `LogisticRegression`, что компонует в себе слои `Linear` и `Flatten`. Класс модели должен наследоваться от `tf.Module`. В качестве функции активации используйте `tf.nn.softmax`.
Создайте объект модели и попробуйте с помощью неё классифицировать одно изображение (не забудьте о размерности батча! Она будет равна 1).
"""

class LogisticRegression(tf.Module):
    def __init__(self, input_dim, output_classes):
        super(LogisticRegression, self).__init__()
        self.flatten = FlattenLayer()
        self.linear = Linear(input_dim, output_classes)

    def __call__(self, inputs):
        flattened = self.flatten(inputs)
        logits = self.linear(flattened)
        probabilities = tf.nn.softmax(logits)
        return probabilities

# Создание объекта модели
model = LogisticRegression(input_dim=28 * 28, output_classes=10)  # 28*28 - размер изображения в Fashion MNIST, 10 классов

# Попытка классификации одного изображения (размер батча = 1)
one_image = X_test[0][None, ...]  # Выбираем первое изображение и добавляем размерность батча

# Приведение типа данных к float32
one_image = tf.cast(one_image, tf.float32)

predictions = model(one_image)
predicted_class = tf.argmax(predictions, axis=1).numpy()[0]

# Вывод предсказанной метки класса и отображение изображения
print("Предсказанный класс для изображения:", predicted_class)

plt.figure()
plt.imshow(X_test[0], cmap='gray')  # Отображение изображения из Fashion MNIST
plt.title(f"Реальный класс: {y_test[0]}")
plt.grid(False)
plt.show()

"""7 Напишите функцию оценки точности модели `evaluate`. Список её аргументов: тестовый tf.Dataset, модель. Данная функция будет отличаться оной из лекций тем, что внутри неё не будет ручной выборки батчей из датасета, поскольку tf.Dataset уже делает это за нас."""

def evaluate(test_dataset, model):
    total_correct = 0
    total_samples = 0

    for images, labels in test_dataset:
        # Приводим тип данных изображений к float32
        images = tf.cast(images, dtype=tf.float32)

        predictions = model(images)
        predicted_labels = tf.argmax(predictions, axis=1)

        # Приводим тип данных меток к int64
        labels = tf.cast(labels, dtype=tf.int64)

        total_samples += labels.shape[0]
        total_correct += tf.reduce_sum(tf.cast(tf.equal(predicted_labels, labels), dtype=tf.int64))


    accuracy = total_correct / total_samples
    return accuracy.numpy()

# Оценка точности модели на тестовом датасете
accuracy = evaluate(test_dataset, model)
print(f"Точность модели на тестовом наборе данных: {accuracy * 100:.2f}%")

"""8 Напишите функцию обучающего шага `train_step`. Данная функция будет немного отличаться от показанной в лекции тем, что ей на вход также дается функция ошибки. Полный список аргументов: модель, тупл данных (батч картинок, батч меток), оптимизатор, функция ошибки. Функция `train_step` должна возвращать вычисленное значение функции ошибки.

В данной лабораторной функция ошибки не будет писаться заново, будет использоваться готовая кросс-энтропия из Keras.
"""

def train_step(model, data, optimizer, loss_fn):
    # Распаковываем данные
    images, labels = data
    # Приводим тип данных изображений к float32
    images = tf.cast(images, dtype=tf.float32)

    # Открываем GradientTape, чтобы записать операции градиентов
    with tf.GradientTape() as tape:
        # Получаем прогнозы модели
        predictions = model(images)
        # Вычисляем значение функции ошибки
        loss = loss_fn(labels, predictions)

    # Вычисляем градиенты относительно функции ошибки и весов модели
    gradients = tape.gradient(loss, model.trainable_variables)
    # Применяем градиенты к весам модели с помощью оптимизатора
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))

    return loss


# Определение функции ошибки (кросс-энтропия из Keras)
loss_function = tf.keras.losses.SparseCategoricalCrossentropy()

# Оптимизатор (например, Adam)
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)

# Проходим по нескольким батчам из обучающего датасета и обновляем веса модели
for batch in train_dataset.take(5):  # Пример: обучаем только на первых 5 батчах
    train_loss = train_step(model, batch, optimizer, loss_function)
    print(f"Batch Loss: {train_loss.numpy()}")

"""9 Оцените точность модели."""

# Оценка точности модели на тестовом датасете
accuracy = evaluate(test_dataset, model)
print(f"Точность модели на тестовом наборе данных: {accuracy * 100:.2f}%")

"""10 Подготовьте гиперпараметры обучения: оптимизатор, функция ошибки ([tf.keras.losses.SparseCategoricalCrossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy)), количество эпох."""

# Гиперпараметры
learning_rate = 0.001
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()
num_epochs = 10  # Количество эпох

"""11 Напишите обучающий цикл. Цикл состоит из внешнего цикла, что отсчитывает эпохи, и вложенного, что итерируется по обучающему датасету.
Во вложенном цикле должна вызываться функция `train_step`. Каждую эпоху модель должна тестироваться функцией `evaluate`. Должны выводиться номер эпохи, значение ошибки и точности. Сохраняйте значения ошибки и точности в списки. Обучите модель. По ходу обучения подбирайте более оптимальные гиперпараметры обучения (learning_rate, другой оптимизатор).
"""

# Списки для сохранения значений ошибки и точности
train_losses = []
train_accuracies = []
test_accuracies = []

# Обучение
for epoch in range(num_epochs):
    epoch_loss = 0.0

    # Обучающий цикл
    for batch in train_dataset:
        loss = train_step(model, batch, optimizer, loss_fn)
        epoch_loss += loss

    epoch_loss /= len(train_dataset)
    train_losses.append(epoch_loss)

    # Оценка точности на тренировочном и тестовом датасетах
    train_accuracy = evaluate(train_dataset, model)
    test_accuracy = evaluate(test_dataset, model)

    train_accuracies.append(train_accuracy)
    test_accuracies.append(test_accuracy)

    print(f"Epoch {epoch + 1}, Loss: {epoch_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Test Accuracy: {test_accuracy:.4f}")

"""12 Выведите графики точности и ошибки, используя matplotlib."""

# Вывод графиков точности и ошибки
plt.figure(figsize=(10, 5))

# График ошибки
plt.subplot(1, 2, 1)
plt.plot(train_losses, label='Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss')
plt.legend()

# График точности
plt.subplot(1, 2, 2)
plt.plot(test_accuracies, label='Test Accuracy', color='orange')
plt.plot(train_accuracies, label='Train Accuracy', color='green')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Accuracies')
plt.legend()

plt.tight_layout()
plt.show()

"""13 (Опционально) Постройте и обучите произвольную нейронную сеть с использованием Keras для решения той же задачи."""

from tensorflow.keras import layers, models

model = models.Sequential([
    layers.Flatten(input_shape=(28, 28)),  # Слой Flatten для преобразования вектора
    layers.Dense(128, activation='relu'),  # Полносвязный слой с 128 нейронами и функцией активации ReLU
    layers.Dense(10, activation='softmax')  # Выходной слой с 10 нейронами (количество классов) и softmax
])

# Компиляция модели
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Обучение модели
history = model.fit(X_train_normalized, y_train, epochs=10, validation_data=(X_test_normalized, y_test))

# Визуализация процесса обучения
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 4))

# График потерь
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()

# График точности
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy')
plt.legend()

plt.tight_layout()
plt.show()